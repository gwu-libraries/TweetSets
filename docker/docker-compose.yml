version: '2'
services:
  # This has 2 ElasticSearch nodes. Scale as necessary to additional nodes.
  elasticsearch1:
      image: docker.elastic.co/elasticsearch/elasticsearch:5.4.0
      container_name: elasticsearch1
      environment:
        - cluster.name=tweetsets-cluster
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
        - xpack.security.enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      # This value is suitable for dev. Set higher for production.
      mem_limit: 1g
      volumes:
        - /tweetsets_data/elasticsearch/esdata1:/usr/share/elasticsearch/data
      ports:
        # TODO: Does this need to be exposed
        - 9200:9200
      networks:
        - tweetsets
  elasticsearch2:
      image: docker.elastic.co/elasticsearch/elasticsearch:5.4.0
      container_name: elasticsearch2
      environment:
        - cluster.name=tweetsets-cluster
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
        - "discovery.zen.ping.unicast.hosts=elasticsearch1"
        - xpack.security.enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      # This value is suitable for dev. Set higher for production.
      mem_limit: 1g
      volumes:
        - /tweetsets_data/elasticsearch/esdata2:/usr/share/elasticsearch/data
      networks:
        - tweetsets
  redis:
      image: redis:3.0.7
      command: --appendonly yes
      volumes:
          - /tweetsets_data/redis:/data
      ports:
          # TODO: Do these need to be exposed?
          - 6379:6379
      networks:
        - tweetsets
  server-flaskrun:
      build:
          context: ..
          dockerfile: Dockerfile-flaskrun
      links:
        - redis:redis
        - elasticsearch1:elasticsearch
      ports:
          - 5000:5000
      volumes:
        - /tweetsets_data/datasets:/tweetsets_data/datasets
      networks:
        - tweetsets
      environment:
        - SECRET_KEY="tweettweet"
  # This can be duplicated for multiple workers
  worker1:
      build:
          context: ..
          dockerfile: Dockerfile-worker
      links:
        - redis:redis
        - elasticsearch1:elasticsearch
      volumes:
        - /tweetsets_data/datasets:/tweetsets_data/datasets
      networks:
        - tweetsets
      environment:
        - LOGGING_LEVEL=INFO

  # This will exit. That's OK.
  loader:
      build:
          context: ..
          dockerfile: Dockerfile-loader
      links:
        - elasticsearch1:elasticsearch
      volumes:
        - ${DATASET_PATH}:/dataset
      networks:
        - tweetsets
      # export DATASET_PATH=../test_data
      # docker-compose run -v test_data:/dataset --rm loader /bin/bash
networks:
  tweetsets:
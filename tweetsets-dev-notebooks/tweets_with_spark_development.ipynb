{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the `loader` module by doing more with Spark\n",
    "#### Goals\n",
    "1. Create full extracts at time of load\n",
    "2. Improve performance of loader (if possible)\n",
    "3. Use Spark to create user-defined extracts\n",
    "\n",
    "#### Approach\n",
    "1. Migrate from Spark RDD to Spark Dataframes to optimize load time.\n",
    "2. Use the initial Spark DF to persist the full extract to disk.\n",
    "2. Leverage Spark Streaming to create user-defined extracts with checkpoints (to make jobs more fault tolerant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `models.py` to create the index in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./TweetSets')\n",
    "from models import to_tweet\n",
    "import models\n",
    "from twarc import json2csv\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is replicated from `loader.py`; it contains some logic that is employed in the RDD load to Elasticsearch (used here for testing/comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_dict(tweet_dict):\n",
    "    new_tweet_dict = tweet_dict['_source']\n",
    "    new_tweet_dict['created_at'] = tweet_dict['_source']['created_at'].isoformat()\n",
    "    new_tweet_dict['tweet_id'] = tweet_dict['_id']\n",
    "    return new_tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `.jar` file is included in the `Spark-loader` Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_jar = '/Users/dsmith/Documents/code/tweetsets-dev/Tweetsets-upgrade-sprint/elasticsearch-hadoop-7.9.2.jar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following environment variable is required for launching the job from the Jupyter notebook. I don't think we need it in production, since it's included in the shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--jars {path_to_jar} pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is also Jupyter-specific; it points the Python kernel to the local Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pyspark` imports contain the functions and classes we need to using the Dataframe API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, explode\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect to our local ES instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch([{'host': 'localhost', 'port': 9200}])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the ES connection in order to create the index, etc.\n",
    "# Running locally -- need to remove the options to sniff on start and retry on timeout\n",
    "from elasticsearch_dsl.connections import connections\n",
    "connections.create_connection(hosts=['localhost:9200'], timeout=90, maxsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import A, Search\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample for testing contains about 40G worth of Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a sample of about 40 GB\n",
    "from pathlib import Path\n",
    "path_to_datasets = Path('./datasets/brexit/sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Spark sessionm and set the TZ to match our Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TweetSets').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two separate indices in ES to compare the performance and result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id_df = 'brexit-test'\n",
    "dataset_id_rdd = 'brexit-test-rdd'\n",
    "def to_tweet_dict(tweet_str):\n",
    "    return clean_tweet_dict(to_tweet(json.loads(tweet_str), dataset_id, '', store_tweet=True).to_dict(include_meta=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Spark RDD, the Dataframe API requires a schema (Scala types). Spark can infer this directly from the `jsonl` documents, but it is more efficient to create the schema in advance and load it at time of processing. Here we create a schema based on a single file and save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = path_to_datasets / '1e30ce0543954d99b169c12390599aea-20191016224305252-00000-mjsaekx2.json'\n",
    "df = spark.read.json(str(path_to_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_schema.json', 'w') as f:\n",
    "    json.dump(schema.jsonValue(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Schema Issues\n",
    "1. We may need to a `full_text` manually to the schema, since it doesn't appear here. (Its presence may depend on how the tweets were harvested.)\n",
    "2. The output for `created_at` is not quite the same as from the Python code. \n",
    "  - Python produces `2019-10-16T22:43:00+00:00`\n",
    "  - Spark SQL produces `2019-10-16T22:43:00Z` where the zero is rendered as `Z`. Not sure if that is okay for Elasticsearch or will make a difference in indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replicating the TS `TweetDocument` Model\n",
    "\n",
    "The following translates the Python code in the `models.to_tweet` function to a Spark SQL statement. Using Spark SQL syntax, some repetition is unavoidable. But it provides optimized performance relative to passing `to_tweet` as a UDF (user-defined function) to Spark for execution. \n",
    "\n",
    "Using syntax for [common table expressions](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-cte.html), we create a CTE with the `tweet_type` as a field, since these velues are used to derive the values in the `urls` and `text` fields. \n",
    "\n",
    "We include some complex columns in the initial SQL expression -- `quoted_status`, `retweeted_status` -- which can be dropped from the final DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_code = '''\n",
    "     with cte as (\n",
    "         select id_str as tweet_id,\n",
    "            case when isnotnull(in_reply_to_status_id) then 'reply'\n",
    "                when isnotnull(retweeted_status) then 'retweet'\n",
    "                when isnotnull(quoted_status) then 'quote'\n",
    "                else 'original'\n",
    "            end as tweet_type,\n",
    "            coalesce(extended_tweet.full_text, text) as text_str,\n",
    "            quoted_status,\n",
    "            retweeted_status,\n",
    "            in_reply_to_user_id_str as in_reply_to_user_id,\n",
    "            in_reply_to_screen_name,\n",
    "            in_reply_to_status_id_str as in_reply_to_status_id,\n",
    "            date_format(to_timestamp(created_at, 'EEE MMM dd HH:mm:ss ZZZZZ yyyy'),\n",
    "                \"yyyy-MM-dd'T'HH:mm:ssX\") as created_at,\n",
    "            user.id_str as user_id,\n",
    "            user.screen_name as user_screen_name,\n",
    "            user.followers_count as user_follower_count,\n",
    "            user.verified as user_verified,\n",
    "            user.lang as user_language,\n",
    "            user.utc_offset as user_utc_offset,\n",
    "            user.time_zone as user_time_zone,\n",
    "            user.location as user_location,\n",
    "            transform(coalesce(extended_tweet.entities.user_mentions,\n",
    "                                entities.user_mentions), x -> x.id_str) as mention_user_ids,\n",
    "            transform(coalesce(extended_tweet.entities.user_mentions,\n",
    "                                entities.user_mentions), x -> x.screen_name) as mention_screen_names,\n",
    "            transform(\n",
    "                    coalesce(extended_tweet.entities.hashtags,\n",
    "                        entities.hashtags), x -> lower(x.text)) as hashtags,\n",
    "            favorite_count,\n",
    "            retweet_count,\n",
    "            lang as language,\n",
    "            isnotnull(entities.media) or isnotnull(extended_tweet.entities.media) \n",
    "                    as has_media,\n",
    "            transform(coalesce(extended_tweet.entities.urls,\n",
    "                        entities.urls), x -> coalesce(x.expanded_url, x.url)) as tweet_urls,\n",
    "            isnotnull(geo) or isnotnull(place) or isnotnull(coordinates) as has_geo,\n",
    "            tweet\n",
    "        from tweets)\n",
    "        select \n",
    "            case when tweet_type = 'quote' then array(text_str, \n",
    "                                                    coalesce(quoted_status.extended_tweet.full_text,\n",
    "                                                            quoted_status.text))\n",
    "                when tweet_type = 'retweet' then array(coalesce(retweeted_status.extended_tweet.full_text,\n",
    "                                                            retweeted_status.text))\n",
    "                else array(text_str)\n",
    "            end as text,\n",
    "            case when tweet_type = 'quote' then quoted_status.user.id_str\n",
    "                else retweeted_status.user.id_str\n",
    "            end as retweeted_quoted_user_id,\n",
    "            case when tweet_type = 'quote' then quoted_status.user.screen_name\n",
    "                else retweeted_status.user.screen_name\n",
    "            end as retweeted_quoted_screen_name,\n",
    "            case when tweet_type = 'quote' then quoted_status.id_str\n",
    "                else retweeted_status.id_str\n",
    "            end as retweet_quoted_status_id,\n",
    "            case when tweet_type = 'quote' then transform(filter(tweet_urls, x -> x not like 'https://twitter.com/%'),\n",
    "                                                            x -> lower(replace(lower(x), 'https://', 'http://')))\n",
    "                else transform(tweet_urls, x -> lower(replace(lower(x), 'https://', 'http://')))\n",
    "            end as urls,\n",
    "            *\n",
    "        from cte\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the SQL to disk for re-use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_sql.sql', 'w') as f:\n",
    "    f.write(sql_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking ETL\n",
    "\n",
    "The following benchmarks were done running Spark in local (not cluster) mode. \n",
    "\n",
    "Machine specs:\n",
    "* MacBook Pro, running Mojave (10.14.6)\n",
    "* 2.8 GHz Intel Core i7, 4 CPU cores\n",
    "* 16 GB 2133 MHz LPDDR3 RAM\n",
    "* 466 GB available of 1 TB storage\n",
    "\n",
    "#### Dataframe vs RDD for local operations\n",
    "\n",
    "Use the `count` method as a simple proxy for operations on large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD, using the current methods in `loader.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'brexit-test-rdd'\n",
    "tweets_str_rdd = spark.sparkContext.textFile('./datasets/brexit/sample/*.json')\n",
    "tweets_rdd = tweets_str_rdd.map(to_tweet_dict).map(lambda row: (row['tweet_id'], row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataframe, applying the transformations necessary for our ES index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7285533"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_full = str(path_to_datasets)\n",
    "df_full = spark.read.schema(schema).json(path_to_full)\n",
    "df_full = df_full.withColumn(\"tweet\", F.to_json(F.struct([df_full[x] for x in df_full.columns])))\n",
    "df_full.createOrReplaceTempView(\"tweets\")\n",
    "df_full = spark.sql(sql_code)\n",
    "cols_to_drop = ['tweet_urls','quoted_status', 'retweeted_status', 'text_str']\n",
    "df_full = df_full.drop(*cols_to_drop)\n",
    "df_full = df_full.withColumn('dataset_id', F.lit(dataset_id))\n",
    "df_full.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method          | Time\n",
    "________________________\n",
    "| RDD             | 24 min\n",
    "\n",
    "| Dataframe       | 50 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking with Elasticsearch Hadoop \n",
    "\n",
    "The following benchmarks were established on a two-node Elasticsearch cluster running locally in Docker. One limitation is that only one node was used by this cluster for loading. Normally, the `elasticsearch-hadoop` library will delegate all available nodes to Spark for reading/writing, but in this case, on account of my configuration, I had to constrain Spark to the primary node. That no doubt introduced additional latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function replicates the logic above for creating a TweetSet document via Spark Dataframe transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spark_df(path_to_datasets, schema, sql_code, dataset_id):\n",
    "    path_to_datasets = str(path_to_datasets)\n",
    "    # Load the JSON into a dataframe, using the supplied schema\n",
    "    df = spark.read.schema(schema).json(path_to_datasets)\n",
    "    # Create a column to hold the JSON representation of the entire document \n",
    "    df = df.withColumn(\"tweet\", F.to_json(F.struct([df[x] for x in df.columns])))\n",
    "    # Register a temp view to execute the SQL code against\n",
    "    df.createOrReplaceTempView(\"tweets\")\n",
    "    # Execute SQL code\n",
    "    df = spark.sql(sql_code)\n",
    "    # Drop intermediate columns\n",
    "    cols_to_drop = ['tweet_urls','quoted_status', 'retweeted_status', 'text_str']\n",
    "    df = df.drop(*cols_to_drop)\n",
    "    # Add dataset ID as a new column\n",
    "    return df.withColumn('dataset_id', F.lit(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating the ES index, we need to register the number of shards. This code is replicated from `tweetset_loader.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_count(tweet_count, store_tweet=True):\n",
    "    # In testing, 500K tweets (storing tweet) = 615MB\n",
    "    # Thus, 32.5 million tweets per shard to have a max shard size of 40GB\n",
    "    # In testing, 500k tweets (not storing tweet) = 145MB\n",
    "    # Thus, 138 million tweets per shard to have a max shard size of 40GB\n",
    "    tweets_per_shard = 32500000 if store_tweet else 138000000\n",
    "    return math.ceil(float(tweet_count) / tweets_per_shard) or 1\n",
    "tweets_per_shard = 32500000\n",
    "tweet_count = 7285533\n",
    "shards = max(shard_count(tweet_count, store_tweet=True), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is also from `tweetset_loader.py`. It creates the index in Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset_id):\n",
    "    # Create the index in ES\n",
    "    tweet_index = models.TweetIndex(dataset_id, shards=shards, replicas=0, refresh_interval=-1)\n",
    "    tweet_index.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_id_df = 'brexit-test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(dataset_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ES-Hadoop configuration, taken from `tweetset_loader.py`. The entry for `es.nodes.wan.only` is used to restrict Spark to a single node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    " es_conf = {\"es.nodes\": 'localhost',\n",
    "                       \"es.port\": \"9200\",\n",
    "                       \"es.index.auto.create\": \"false\",\n",
    "                       \"es.mapping.id\": \"tweet_id\",\n",
    "                       \"es.resource\": dataset_id_df,\n",
    "                       'es.nodes.discovery': 'false',\n",
    "                       'es.nodes.data.only': 'false',\n",
    "                       \"es.nodes.wan.only\": \"true\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Spark Dataframe. (Won't actually execute until the `save` operation below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = make_spark_df(path_to_datasets, schema, sql_code, dataset_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7285533"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "df_full.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Spark DataFrame to ES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.write.format('org.elasticsearch.spark.sql').options(**es_conf).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total time spent: 3.7 hours\n",
    "\n",
    "![Job report from Spark Job monitor for DF: 3.7 hours](spark-es-df.png)\n",
    "![Job stats from Spark Jobs for DF](spark-es-df-stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for comparison, we load the same dataset as a separate index, using the RDD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(dataset_id_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = dataset_id_rdd\n",
    "tweets_str_rdd = spark.sparkContext.textFile('./datasets/brexit/sample/*.json')\n",
    "tweets_rdd = tweets_str_rdd.map(to_tweet_dict).map(lambda row: (row['tweet_id'], row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same settings as above, except for the value of `es.resource` (the ES index name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    " es_conf = {\"es.nodes\": 'localhost',\n",
    "                       \"es.port\": \"9200\",\n",
    "                       \"es.index.auto.create\": \"false\",\n",
    "                       \"es.mapping.id\": \"tweet_id\",\n",
    "                       \"es.resource\": dataset_id_rdd,\n",
    "                       'es.nodes.discovery': 'false',\n",
    "                       'es.nodes.data.only': 'false',\n",
    "                       \"es.nodes.wan.only\": \"true\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is verbatim from `tweetset_loader.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_rdd.saveAsNewAPIHadoopFile(\n",
    "                path='-',\n",
    "                outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "                keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "                valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "                conf=es_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total time spent: 4.9 hours\n",
    "\n",
    "![Job report from Spark Jobs for RDD: 4.9 hours](spark-es-rdd.png)\n",
    "![Job stats from Spark Jobs for RDD](spark-es-rdd-stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "The Dataframe API performs about 30% better than the RDD API in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing ES schema \n",
    "\n",
    "Comparing the ES documents created via Spark SQL with those created by the current method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified version of this TweetSets method that does not use the TweetSets index naming convention. See the commented line below:\n",
    "\n",
    "```\n",
    "    index = dataset_params.get('source_dataset')\n",
    "    #index = get_tweets_index_name(source_dataset)\n",
    "    search = Search(index=index).extra(track_total_hits=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_mod import dataset_params_to_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare aggregate results, using the date field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_aggs(dataset_rdd, dataset_df):\n",
    "    for d in (dataset_rdd, dataset_df):\n",
    "        s = Search(using=es, index=dataset_id_df)\n",
    "        s.aggs.bucket('dates', 'stats', field='created_at') \n",
    "        response = s.execute()\n",
    "        print(f'Aggregation from {d}')\n",
    "        print(response.aggregations.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation from brexit-test-rdd\n",
      "{'dates': {'count': 7284216, 'min': 1571265780000.0, 'max': 1572311576000.0, 'avg': 1571737589951.189, 'sum': 1.144887610052389e+19, 'min_as_string': '2019-10-16T22:43:00.000Z', 'max_as_string': '2019-10-29T01:12:56.000Z', 'avg_as_string': '2019-10-22T09:46:29.951Z', 'sum_as_string': '+292278994-08-17T07:12:55.807Z'}}\n",
      "Aggregation from brexit-test\n",
      "{'dates': {'count': 7284216, 'min': 1571265780000.0, 'max': 1572311576000.0, 'avg': 1571737589951.189, 'sum': 1.144887610052389e+19, 'min_as_string': '2019-10-16T22:43:00.000Z', 'max_as_string': '2019-10-29T01:12:56.000Z', 'avg_as_string': '2019-10-22T09:46:29.951Z', 'sum_as_string': '+292278994-08-17T07:12:55.807Z'}}\n"
     ]
    }
   ],
   "source": [
    "compare_aggs(dataset_id_rdd, dataset_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to retrieve documents from ES index up to certain limit. Returns both Tweet ID's and CSV representation of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(dataset_params, n=1000):\n",
    "    s = dataset_params_to_search(dataset_params, skip_aggs=True)\n",
    "    results = [r for i, r in enumerate(s.scan()) if i < n]\n",
    "    ids = [r.meta.id for r in results]\n",
    "    csv = [json2csv.get_row(json.loads(hit.tweet), excel=True) for hit in results]\n",
    "    return ids, csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compare the CSV representation of two sets of Tweet documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_csvs(csv1, csv2):\n",
    "    for i, (c1, c2) in enumerate(zip(sorted(csv1, key=lambda x: x[0]),\n",
    "                                     sorted(csv2, key=lambda x: x[0]))):\n",
    "        for r1, r2 in zip(c1, c2):\n",
    "            try:\n",
    "                assert r1 == r2\n",
    "            except AssertionError:\n",
    "                print(f'Row {i}, CSV1 has {r1}, CSV2 has {r2}')\n",
    "                continue\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to execute the same search against two indices and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tests(dataset_params, dataset_1, dataset_2, n=1000):\n",
    "    dataset_params['source_dataset'] = dataset_1\n",
    "    ids1, csv1 = retrieve_docs(dataset_params, n)\n",
    "    dataset_params['source_dataset'] = dataset_2\n",
    "    ids2, csv2 = retrieve_docs(dataset_params, n)\n",
    "    print(f'Comparing the first {n} of {len(ids1)} Tweet IDs from {dataset_1} and {dataset_2}')\n",
    "    try:\n",
    "        assert set(ids1) - set(ids2) == set()\n",
    "    except AssertionError:\n",
    "        print(f'Tweet IDs returned do not match.')\n",
    "    print(f'Comparing CSV results from {dataset_1} and {dataset_2}')\n",
    "    compare_csvs(csv1, csv2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic search by date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params_date_range = {\n",
    "    'tweet_type_original': 'true',\n",
    "    'created_at_from': '2019-10-16',\n",
    "    'created_at_to': '2019-10-17'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the first 1000 of 104131 Tweet IDs from brexit-test-rdd and brexit-test\n",
      "Comparing CSV results from brexit-test-rdd and brexit-test\n"
     ]
    }
   ],
   "source": [
    "do_tests(dataset_params_date_range, dataset_id_rdd, dataset_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the first 1000 of 62129 Tweet IDs from brexit-test-rdd and brexit-test\n",
      "Comparing CSV results from brexit-test-rdd and brexit-test\n"
     ]
    }
   ],
   "source": [
    "dataset_params_keyword = {\n",
    "    'tweet_text_all': 'UK',\n",
    "    'tweet_type_original': 'true'\n",
    "}\n",
    "do_tests(dataset_params_keyword, dataset_id_rdd, dataset_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "The ES schemas (RDD-derived vs. Dataframe-derived) yield identical results on keyword and date-range searches, as well as on aggregation by date. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible issuse with JSON representation of Tweet\n",
    "\n",
    "It is [a known issue](https://stackoverflow.com/questions/60008665/why-spark-to-json-not-populating-null-values) in Spark v.2 that the Dataframe API in serializing to JSON will drop null fields. This poses a problem for creating the `tweet` field in the Tweet document, which contains the full JSON representation of the Tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the initial load from disk, the null fields in the JSON are preserved. Note the `contributors` and `coordinates` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(contributors=None, coordinates=None, created_at='Mon Oct 28 14:18:15 +0000 2019', display_text_range=None, entities=Row(hashtags=[Row(indices=[91, 95], text='Ads'), Row(indices=[96, 104], text='apology'), Row(indices=[105, 112], text='brexit')], media=None, symbols=[], urls=[Row(display_url='flyingeze.com/?p=15306', expanded_url='https://flyingeze.com/?p=15306', indices=[67, 90], url='https://t.co/e21kQGAYMM'), Row(display_url='twitter.com/i/web/status/1…', expanded_url='https://twitter.com/i/web/status/1188822287841034240', indices=[114, 137], url='https://t.co/x9Xb4OMeJq')], user_mentions=[Row(id=3081529131, id_str='3081529131', indices=[56, 66], name='Flying Eze', screen_name='flyingeze')]), extended_entities=None, extended_tweet=Row(display_text_range=[0, 147], entities=Row(hashtags=[Row(indices=[91, 95], text='Ads'), Row(indices=[96, 104], text='apology'), Row(indices=[105, 112], text='brexit'), Row(indices=[113, 121], text='Dickens'), Row(indices=[122, 131], text='facebook'), Row(indices=[132, 141], text='politics'), Row(indices=[142, 147], text='News')], media=None, symbols=[], urls=[Row(display_url='flyingeze.com/?p=15306', expanded_url='https://flyingeze.com/?p=15306', indices=[67, 90], url='https://t.co/e21kQGAYMM')], user_mentions=[Row(id=3081529131, id_str='3081529131', indices=[56, 66], name='Flying Eze', screen_name='flyingeze')]), extended_entities=None, full_text='Facebook and politics; Brexit ads; Dickens — an apology @flyingeze\\nhttps://t.co/e21kQGAYMM\\n#Ads #apology #brexit #Dickens #facebook #politics #News'), favorite_count=0, favorited=False, filter_level='low', geo=None, id=1188822287841034240, id_str='1188822287841034240', in_reply_to_screen_name=None, in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, is_quote_status=False, lang='en', place=None, possibly_sensitive=False, quote_count=0, quoted_status=None, quoted_status_id=None, quoted_status_id_str=None, quoted_status_permalink=None, reply_count=0, retweet_count=0, retweeted=False, retweeted_status=None, source='<a href=\"https://flyingeze.com/\" rel=\"nofollow\">Flying Eze</a>', text='Facebook and politics; Brexit ads; Dickens — an apology @flyingeze\\nhttps://t.co/e21kQGAYMM\\n#Ads #apology #brexit… https://t.co/x9Xb4OMeJq', timestamp_ms='1572272295328', truncated=True, user=Row(contributors_enabled=False, created_at='Mon Mar 09 13:22:41 +0000 2015', default_profile=False, default_profile_image=False, description='The Home of #Sports, #Entertainments, #News and #Lifestyle updates, with #Loan, #Technology, & #Health highlights and much more from across the world.', favourites_count=432, follow_request_sent=None, followers_count=467, following=None, friends_count=1409, geo_enabled=False, id=3081529131, id_str='3081529131', is_translator=False, lang=None, listed_count=5, location=None, name='Flying Eze', notifications=None, profile_background_color='000000', profile_background_image_url='http://abs.twimg.com/images/themes/theme1/bg.png', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_tile=False, profile_banner_url='https://pbs.twimg.com/profile_banners/3081529131/1565907495', profile_image_url='http://pbs.twimg.com/profile_images/1086021490162757632/eePFoWxq_normal.png', profile_image_url_https='https://pbs.twimg.com/profile_images/1086021490162757632/eePFoWxq_normal.png', profile_link_color='19CF86', profile_sidebar_border_color='000000', profile_sidebar_fill_color='000000', profile_text_color='000000', profile_use_background_image=False, protected=False, screen_name='flyingeze', statuses_count=62877, time_zone=None, translator_type='none', url='https://flyingeze.com/', utc_offset=None, verified=False), withheld_in_countries=None)]"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = spark.read.schema(schema).json(str(path_to_datasets))\n",
    "id_str = '1188822287841034240'\n",
    "df1 = df_all.filter(df_all.id_str == id_str)\n",
    "df1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the operation proposed to render the Tweet as a valid JSON string in the `tweet` field. Note that the `contributor` and `coordinates` fields are missing completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tweet='{\"created_at\":\"Mon Oct 28 14:18:15 +0000 2019\",\"entities\":{\"hashtags\":[{\"indices\":[91,95],\"text\":\"Ads\"},{\"indices\":[96,104],\"text\":\"apology\"},{\"indices\":[105,112],\"text\":\"brexit\"}],\"symbols\":[],\"urls\":[{\"display_url\":\"flyingeze.com/?p=15306\",\"expanded_url\":\"https://flyingeze.com/?p=15306\",\"indices\":[67,90],\"url\":\"https://t.co/e21kQGAYMM\"},{\"display_url\":\"twitter.com/i/web/status/1…\",\"expanded_url\":\"https://twitter.com/i/web/status/1188822287841034240\",\"indices\":[114,137],\"url\":\"https://t.co/x9Xb4OMeJq\"}],\"user_mentions\":[{\"id\":3081529131,\"id_str\":\"3081529131\",\"indices\":[56,66],\"name\":\"Flying Eze\",\"screen_name\":\"flyingeze\"}]},\"extended_tweet\":{\"display_text_range\":[0,147],\"entities\":{\"hashtags\":[{\"indices\":[91,95],\"text\":\"Ads\"},{\"indices\":[96,104],\"text\":\"apology\"},{\"indices\":[105,112],\"text\":\"brexit\"},{\"indices\":[113,121],\"text\":\"Dickens\"},{\"indices\":[122,131],\"text\":\"facebook\"},{\"indices\":[132,141],\"text\":\"politics\"},{\"indices\":[142,147],\"text\":\"News\"}],\"symbols\":[],\"urls\":[{\"display_url\":\"flyingeze.com/?p=15306\",\"expanded_url\":\"https://flyingeze.com/?p=15306\",\"indices\":[67,90],\"url\":\"https://t.co/e21kQGAYMM\"}],\"user_mentions\":[{\"id\":3081529131,\"id_str\":\"3081529131\",\"indices\":[56,66],\"name\":\"Flying Eze\",\"screen_name\":\"flyingeze\"}]},\"full_text\":\"Facebook and politics; Brexit ads; Dickens — an apology @flyingeze\\\\nhttps://t.co/e21kQGAYMM\\\\n#Ads #apology #brexit #Dickens #facebook #politics #News\"},\"favorite_count\":0,\"favorited\":false,\"filter_level\":\"low\",\"id\":1188822287841034240,\"id_str\":\"1188822287841034240\",\"is_quote_status\":false,\"lang\":\"en\",\"possibly_sensitive\":false,\"quote_count\":0,\"reply_count\":0,\"retweet_count\":0,\"retweeted\":false,\"source\":\"<a href=\\\\\"https://flyingeze.com/\\\\\" rel=\\\\\"nofollow\\\\\">Flying Eze</a>\",\"text\":\"Facebook and politics; Brexit ads; Dickens — an apology @flyingeze\\\\nhttps://t.co/e21kQGAYMM\\\\n#Ads #apology #brexit… https://t.co/x9Xb4OMeJq\",\"timestamp_ms\":\"1572272295328\",\"truncated\":true,\"user\":{\"contributors_enabled\":false,\"created_at\":\"Mon Mar 09 13:22:41 +0000 2015\",\"default_profile\":false,\"default_profile_image\":false,\"description\":\"The Home of #Sports, #Entertainments, #News and #Lifestyle updates, with #Loan, #Technology, & #Health highlights and much more from across the world.\",\"favourites_count\":432,\"followers_count\":467,\"friends_count\":1409,\"geo_enabled\":false,\"id\":3081529131,\"id_str\":\"3081529131\",\"is_translator\":false,\"listed_count\":5,\"name\":\"Flying Eze\",\"profile_background_color\":\"000000\",\"profile_background_image_url\":\"http://abs.twimg.com/images/themes/theme1/bg.png\",\"profile_background_image_url_https\":\"https://abs.twimg.com/images/themes/theme1/bg.png\",\"profile_background_tile\":false,\"profile_banner_url\":\"https://pbs.twimg.com/profile_banners/3081529131/1565907495\",\"profile_image_url\":\"http://pbs.twimg.com/profile_images/1086021490162757632/eePFoWxq_normal.png\",\"profile_image_url_https\":\"https://pbs.twimg.com/profile_images/1086021490162757632/eePFoWxq_normal.png\",\"profile_link_color\":\"19CF86\",\"profile_sidebar_border_color\":\"000000\",\"profile_sidebar_fill_color\":\"000000\",\"profile_text_color\":\"000000\",\"profile_use_background_image\":false,\"protected\":false,\"screen_name\":\"flyingeze\",\"statuses_count\":62877,\"translator_type\":\"none\",\"url\":\"https://flyingeze.com/\",\"verified\":false}}')]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.withColumn(\"tweet\", F.to_json(F.struct([df1[x] for x in df1.columns]))).select('tweet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Possible solutions\n",
    "1. Upgrade to Spark v. 3. (I don't think there are any changes that would break the current/proposed implementation, but this would require testing.) It looks like the `elasticsearch-hadoop` library supports Spark 3 as of [version 7.12.0](https://www.elastic.co/guide/en/elasticsearch/hadoop/7.12/eshadoop-7.12.0.html). We are currently using 7.9.2 \n",
    "2. Decide that null fields are not important; users can always account for these when processing the JSON extracts downstream.\n",
    "3. Explore other ways to add the serialized representation to the Tweet document at time of load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating extracts at time of load\n",
    "\n",
    "Since we have loaded the Tweet `jsonl` documents into a Dataframe, we can write to CSV and JSON at load time in order to produce the full extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = Path('./tweetset_data/full_datasets/brexit-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write the Tweet ID's to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.select('tweet_id').write.csv(str(path_to_ids / 'brexit-test-ids'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON serialization of the full Tweet documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.select('tweet').write.json(str(path_to_files / 'brexit-test-json'),\n",
    "                                  compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Porting `twarc.json2csv` to Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping of columns in our implementation to those in json2csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'retweet_quoted_status_id': 'retweet_or_quote_id',\n",
    "    'retweeted_quoted_screen_name': 'retweet_or_quote_screen_name',\n",
    "    'tweet_id': 'id',\n",
    "    'user_follower_count': 'user_followers_count',\n",
    "    'language': 'lang',\n",
    "    'retweeted_quoted_user_id': 'retweet_or_quote_user_id'\n",
    "}\n",
    "column_mapping.update({k: k for k in df_full.columns if k in json2csv.get_headings()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `text`, `hashtags` and `urls`, we need to convert from array to space-separated string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional transformations required.\n",
    "\n",
    "- tweet_url \n",
    "```\n",
    "\"https://twitter.com/%s/status/%s\" % (t[\"user\"][\"screen_name\"], t[\"id_str\"])\n",
    "```\n",
    "- parsed_created_at\n",
    "```\n",
    "date_parse(get(\"created_at\"))\n",
    "```\n",
    "- coordinates\n",
    "```\n",
    "    \"%f %f\" % tuple(t[\"coordinates\"][\"coordinates\"])\n",
    "```\n",
    "- media\n",
    "```\n",
    "if \"extended_entities\" in t and \"media\" in t[\"extended_entities\"]:\n",
    "        return \" \".join([h[\"media_url_https\"] for h in t[\"extended_entities\"][\"media\"]])\n",
    "elif \"media\" in t[\"entities\"]:\n",
    "        return \" \".join([h[\"media_url_https\"] for h in t[\"entities\"][\"media\"]])\n",
    "else:\n",
    "     return None\n",
    "```\n",
    "- place\n",
    "```\n",
    "    if \"place\" in t and t[\"place\"]:\n",
    "        return t[\"place\"][\"full_name\"]\n",
    "```\n",
    "- possible_sensitive\n",
    "```\n",
    "t.get('possibly_sensitive')\n",
    "```\n",
    "- source\n",
    "```\n",
    "t.get('source')\n",
    "```\n",
    "- user_created_at\n",
    "```\n",
    " t.get(\"user\").get('created_at')\n",
    "```\n",
    "- user_default_profile_image\n",
    "```\n",
    "t.get(\"user\").get('default_profile_image')\n",
    "```\n",
    "- user_description\n",
    "```\n",
    "t.get(\"user\").get('description').replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "```\n",
    "- user_favourites_count\n",
    "```\n",
    "t.get(\"user\").get('favorites_count')\n",
    "```\n",
    "- user_friends_count\n",
    "```\n",
    "t.get(\"user\").get('friends_count')\n",
    "```\n",
    "- user_listed_count\n",
    "```\n",
    "t.get(\"user\").get('listed_count')\n",
    "```\n",
    "- user_name\n",
    "```\n",
    "t.get(\"user\").get('name').replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "```\n",
    "- user_statuses_count\n",
    "```\n",
    "t.get(\"user\").get('statuses_count')\n",
    "```\n",
    "- user_urls\n",
    "\n",
    "**Question**: Does the `user` field still contain an `entities` element? It doesn't show up in the schema derived from the Brexit Tweets\n",
    "```\n",
    " u = t.get(\"user\")\n",
    "    if not u:\n",
    "        return None\n",
    "    urls = []\n",
    "    if \"entities\" in u and \"url\" in u[\"entities\"] and \"urls\" in u[\"entities\"][\"url\"]:\n",
    "        for url in u[\"entities\"][\"url\"][\"urls\"]:\n",
    "            if url[\"expanded_url\"]:\n",
    "                urls.append(url[\"expanded_url\"])\n",
    "    return \" \".join(urls)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised SQL query to include additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tweet_sql_exp.sql') as f:\n",
    "    sql_code_exp = f.read()\n",
    "df_exp = make_spark_df(path_to_datasets, schema, sql_code_exp, dataset_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns for ES \n",
    "\n",
    "**Need to hard code these somewhere**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_columns = df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'retweeted_quoted_user_id',\n",
       " 'retweeted_quoted_screen_name',\n",
       " 'retweet_quoted_status_id',\n",
       " 'urls',\n",
       " 'tweet_id',\n",
       " 'tweet_type',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'created_at',\n",
       " 'user_id',\n",
       " 'user_screen_name',\n",
       " 'user_follower_count',\n",
       " 'user_verified',\n",
       " 'user_language',\n",
       " 'user_utc_offset',\n",
       " 'user_time_zone',\n",
       " 'user_location',\n",
       " 'mention_user_ids',\n",
       " 'mention_screen_names',\n",
       " 'hashtags',\n",
       " 'favorite_count',\n",
       " 'retweet_count',\n",
       " 'language',\n",
       " 'has_media',\n",
       " 'has_geo',\n",
       " 'tweet',\n",
       " 'dataset_id']"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns for CSV extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_columns = json2csv.get_headings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the following expression when saving to ES** to limit to those columns that match our ES schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: array<string>, retweeted_quoted_user_id: string, retweeted_quoted_screen_name: string, retweet_quoted_status_id: string, urls: array<string>, tweet_id: string, tweet_type: string, in_reply_to_user_id: string, in_reply_to_screen_name: string, in_reply_to_status_id: string, created_at: string, user_id: string, user_screen_name: string, user_follower_count: bigint, user_verified: boolean, user_language: string, user_utc_offset: string, user_time_zone: string, user_location: string, mention_user_ids: array<string>, mention_screen_names: array<string>, hashtags: array<string>, favorite_count: bigint, retweet_count: bigint, language: string, has_media: boolean, has_geo: boolean, tweet: string, dataset_id: string]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exp.select(es_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Dataframe for CSV extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(df, column_map):\n",
    "    # Convert array columns to string representations\n",
    "    for c in ['text', 'hashtags', 'urls']:\n",
    "        df = df.withColumn(c, F.concat_ws(' ', df[c]))\n",
    "    # Rename columns as necessary\n",
    "    for k, v in column_map.items():\n",
    "        if k != v:\n",
    "            df = df.withColumnRenamed(k, v)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = make_csv(df_exp, column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, ommitting the possibly superfluous `user_urls` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_columns = [c for c in csv_columns if c != 'user_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.select(csv_columns).write.csv(str(path_to_files / 'brexit-test-csv'),\n",
    "                                  compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare `mentions` (edges and nodes). We use the parsed Dataframe (the version we loaded to ES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(df):\n",
    "    # Create a temp table so that we can use SQL\n",
    "    df.createOrReplaceTempView(\"tweets_parsed\")\n",
    "    # SQL for extracting the mention ids, screen_names, and user_ids\n",
    "    mentions_sql = '''\n",
    "    select mentions.*,\n",
    "        user_id\n",
    "    from (\n",
    "        select \n",
    "            explode(arrays_zip(mention_user_ids, mention_screen_names)) as mentions,\n",
    "            user_id\n",
    "        from tweets_parsed\n",
    "    )\n",
    "    '''\n",
    "    mentions_df = spark.sql(mentions_sql)\n",
    "    mention_edges = mentions_df.select('mention_user_ids', 'mention_screen_names')\\\n",
    "                        .distinct()\n",
    "    mention_nodes = mentions_df.select('mention_user_ids', 'user_id').distinct()\n",
    "    return mention_edges, mention_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, nodes = get_mentions(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.write.csv(str(path_to_files / 'brexit-test-mention-edges'),\n",
    "                                  compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.write.csv(str(path_to_files / 'brexit-test-mention-nodes'),\n",
    "                                  compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare count of mentions per mentioned user (aka `top mentions`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_mentions():\n",
    "    # Reusing the temporary table already registered\n",
    "    sql_agg = '''\n",
    "    select count(distinct tweet_id) as number_mentions,\n",
    "        mention_user_id as mentioned_user\n",
    "    from (\n",
    "        select \n",
    "            explode(mention_user_ids) as mention_user_id,\n",
    "            tweet_id\n",
    "        from tweets_parsed\n",
    "    )\n",
    "    group by mention_user_id\n",
    "    '''\n",
    "    ment_agg_df = spark.sql(sql_agg)\n",
    "    return ment_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_agg_df = agg_mentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_agg_df.write.csv(str(path_to_files / 'brexit-test-mention-counts'),\n",
    "                                  compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "- Count of Tweets: 7,285,533\n",
    "- Size on disk: 49.6 GB\n",
    "--------\n",
    "1. Writing Tweet ID's to gzipped CSV\n",
    "   - Time: 1.2 min\n",
    "   - Size on disk: 58 MB\n",
    "2. Writing full Tweets to gzipped JSON\n",
    "   - Time: 8.1 min\n",
    "   - Size on disk: 6.74 GB\n",
    "3. Writing full Tweets to gzipped CSV\n",
    "   - Time: 4.4 min\n",
    "   - Size on disk: 2.16 GB\n",
    "4. Writing mentions edges to gzipped CSV\n",
    "   - Time: 1.7 min\n",
    "   - Size on disk: 5 MB\n",
    "5. Writing mentions nodes to gzipped CSV\n",
    "   - Time: 1.9 min\n",
    "   - Size on disk: 65.3 MB\n",
    "6. Writing mention counts to gzipped CSV\n",
    "   - Size on disk: 3.3 MB\n",
    "   - Time: 1.9 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieval from ES via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pass as an option a list (string) of the fields with array type in ES\n",
    "read_options = {\"es.nodes\": 'localhost',\n",
    "                       \"es.port\": \"9200\",\n",
    "                       \"es.index.auto.create\": \"false\",\n",
    "                       \"es.mapping.id\": \"tweet_id\",\n",
    "                       \"es.resource\": index_name,\n",
    "                       'es.nodes.discovery': 'false',\n",
    "                       'es.nodes.data.only': 'false',\n",
    "                        'es.read.field.as.array.include': 'text,urls,mention_user_ids,mention_screen_names,hashtags',\n",
    "                       \"es.nodes.wan.only\": \"true\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.format(\"org.elasticsearch.spark.sql\")\\\n",
    "                        .options(**read_options)\\\n",
    "                        .load(\"brexit-test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
